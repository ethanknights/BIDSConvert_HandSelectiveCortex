# Knights, E., Mansfield, C., Tonin, D., Saada, J., Smith, F., & Rossit, S. (2020). Hand-selective visual regions represent how to grasp 3D tools for use: brain decoding during real actions. 


This repository contains code for:

- **BIDS conversion**
https://openneuro.org/datasets/ds003342/versions/1.0.0. See BIDSconversion/convertBIDS.m
- **Real Action Experiment Code**
Use runExperiment/run_MRI_realAction.m on a stimulation pc to read the sound (.wav) and delimited (.xlsx) files, with minilab.m on the matlab path.
- **Stats**
Download MVPA data (https://osf.io/wjnxk/) and use stats/BayesTests.m
- **Plots** 
Download MVPA data (https://osf.io/wjnxk/) and use plots/run_violinPlots.m


**MVPA & searchlight code**
See https://github.com/fws252/ROI-based-and-searchlight-MVPA-decoding-Knights-et-al-2020-
**Visual Localiser Experiment Code**
TBC.


# How to Acknowledge
Please cite: Knights, E., Mansfield, C., Tonin, D., Saada, J., Smith, F., & Rossit, S. (2020). Hand-selective visual regions represent how to grasp 3D tools for use: brain decoding during real actions. bioRxiv. doi: 10.1101/2020.10.14.339606